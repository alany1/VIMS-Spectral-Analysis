{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alanyu/anaconda3/lib/python3.8/site-packages/pysis/env.py:33: RuntimeWarning: Could not find isis. Is `ISISROOT` set?\n",
      "  warnings.warn('Could not find isis. Is `ISISROOT` set?', RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "from IPython.core.display import HTML\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LogNorm\n",
    "from pylab import *\n",
    "import pysis\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "import scipy.optimize as sco\n",
    "import time\n",
    "import ast\n",
    "from tkinter import *\n",
    "from astropy.time import Time\n",
    "%matplotlib qt\n",
    "from IPython.core.display import HTML \n",
    "from PIL import Image\n",
    "vims_wave = np.loadtxt('/home/alanyu/Dropbox (MIT)/VIMS_UROP/vims_wave.txt') \n",
    "HTML(\"<style>.container { width:96% !important; }</style>\") \n",
    "from fitting_routine import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dune_shape_files/Shangrai-La.shp', 'dune_shape_files/Aztlan.shp', 'dune_shape_files/Fensal.shp', 'dune_shape_files/Senkyo.shp', 'dune_shape_files/Belet.shp']\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import shapefile\n",
    "from PIL import Image\n",
    "iss = Image.open(\"Titan_ISS_P19658_Mosaic_Global_4km.tif\")\n",
    "plt.imshow(iss,extent=[0,360,-90,90], cmap='gray')\n",
    "\n",
    "#sfiles = glob.glob(\"dune_shape_files/Belet.shp\")\n",
    "sfiles = glob.glob(\"dune_shape_files/*.shp\")\n",
    "print(sfiles)\n",
    "\n",
    "sfiles = ['dune_shape_files/Shangrai-La.shp', 'dune_shape_files/Senkyo.shp', 'dune_shape_files/Belet.shp']\n",
    "sfiles = ['dune_shape_files/Aztlan.shp', 'dune_shape_files/Fensal.shp']\n",
    "for f in sfiles:\n",
    "    sf = shapefile.Reader(f)\n",
    "    for shape in sf.shapes():\n",
    "        vertices = np.asarray(shape.points)\n",
    "        ind = np.where(vertices[:,0]<0)\n",
    "        vertices[ind,0] = vertices[ind,0]+360\n",
    "        plt.scatter(vertices[:,0],vertices[:,1])\n",
    "        \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "belet = glob.glob(\"pixels/belet/*.pkl\")\n",
    "senkyo = glob.glob(\"pixels/senkyo/*.pkl\")\n",
    "shangrai = glob.glob(\"pixels/shangrai/*.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(info):\n",
    "    \"\"\"\n",
    "    info: spectrum list.\n",
    "    \n",
    "    Returns: Parameters of fitting routine as a dictionary of dictionaries.\n",
    "    \"\"\"\n",
    "    band_channels = [29,30,31,32,47,48,49,50,51,52,53,54,55,84,85,86,87,88,89,90]# + [i for i in range(160,181)] \n",
    "    plaw = False\n",
    "    \n",
    "    fits = {'1.0':{},'1.2':{},'1.6':{},'2.0':{}}\n",
    "    \n",
    "    #attempt to fit powerlaw:\n",
    "    try:\n",
    "        p_fit,p_cov = sco.curve_fit(powerlaw,vims_wave[band_channels],info[band_channels],p0=(0.005,-2,0),maxfev=100000)\n",
    "        p=powerlaw(vims_wave,*p_fit)\n",
    "        adjusted = info-p\n",
    "        plaw=True\n",
    "    except:\n",
    "        adjusted = info\n",
    "    \n",
    "    #2.0:\n",
    "    try:\n",
    "        fit,cov=sco.curve_fit(gaussian,vims_wave[60:85],adjusted[60:85],p0=(2.0,.125,.1),maxfev=100000)\n",
    "        if plaw:\n",
    "            fits['2.0']={'p_amp': p_fit[0],'p_power': p_fit[1],'p_const': p_fit[2],'g_mean':fit[0],'g_sigma': fit[1],'g_amp':fit[2]} \n",
    "        else:\n",
    "            fits['2.0']={'g_mean':fit[0],'g_sigma': fit[1],'g_amp':fit[2]} \n",
    "    except:\n",
    "        return {}\n",
    "    \n",
    "    #1.6:\n",
    "    try:\n",
    "        fit,cov=sco.curve_fit(gaussian,vims_wave[35:54],adjusted[35:54],p0=(1.6,.125,.1),maxfev=100000)\n",
    "        if plaw:\n",
    "            fits['1.6'] = {'p_amp': p_fit[0],'p_power': p_fit[1],'p_const': p_fit[2],'g_mean':fit[0],'g_sigma': fit[1],'g_amp':fit[2]}\n",
    "        else:\n",
    "            fits['1.6']={'g_mean':fit[0],'g_sigma': fit[1],'g_amp':fit[2]} \n",
    "    except:\n",
    "        return {}\n",
    "    \n",
    "    #1.2:\n",
    "    try:\n",
    "        fit,cov=sco.curve_fit(gaussian,vims_wave[12:30],adjusted[12:30],p0=(1.2,.125,.1),maxfev=100000)\n",
    "        if plaw:\n",
    "            fits['1.2'] = {'p_amp': p_fit[0],'p_power': p_fit[1],'p_const': p_fit[2],'g_mean':fit[0],'g_sigma': fit[1],'g_amp':fit[2]}\n",
    "        else:\n",
    "            fits['1.2']={'g_mean':fit[0],'g_sigma': fit[1],'g_amp':fit[2]} \n",
    "    except:\n",
    "        return {}\n",
    "    \n",
    "    try:\n",
    "        fit,cov=sco.curve_fit(gaussian,vims_wave[:18],adjusted[:18],p0=(1.0,.125,.1),maxfev=100000)\n",
    "        if plaw:\n",
    "            fits['1.0'] = {'p_amp': p_fit[0],'p_power': p_fit[1],'p_const': p_fit[2],'g_mean':fit[0],'g_sigma': fit[1],'g_amp':fit[2]}\n",
    "        else:\n",
    "            fits['1.0']={'g_mean':fit[0],'g_sigma': fit[1],'g_amp':fit[2]} \n",
    "    except:\n",
    "        return {}\n",
    "    \n",
    "    fits['2.0']['channel']=info[69]\n",
    "    fits['1.0']['channel']=info[8]\n",
    "    fits['1.2']['channel']=info[20]\n",
    "    fits['1.6']['channel']=info[45]\n",
    "    \n",
    "    return fits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_csv(path_to_csv):\n",
    "    \"\"\"\n",
    "    path_to_csv: path to file\n",
    "        \n",
    "    Returns dictionary containing desired data. Only works for * query.\n",
    "    \"\"\"\n",
    "    count = 0\n",
    "    data = []\n",
    "    shape_name = path_to_csv.split(\"_\")[0]\n",
    "    bad = 0\n",
    "    with open(path_to_csv,'r') as f:\n",
    "        \n",
    "        head = f.readline().replace('\"','').strip().split(',')\n",
    "        print('head',head)\n",
    "        while True:\n",
    "            \n",
    "            try:\n",
    "                entry = f.readline()\n",
    "            except:\n",
    "            #    print('2')\n",
    "                break\n",
    "                \n",
    "            if entry == '':\n",
    "                print('End')\n",
    "                break\n",
    "                \n",
    "            entry=entry.replace('\"','').split('{')\n",
    "            info = entry[0][:-1].split(',')\n",
    "            #print(entry[0], '\\n --- \\n', entry[1])\n",
    "            spectra = list(map(float,entry[1].split('}')[0].split(',')))\n",
    "            #print(temp[0],'\\n ---- \\n', temp[1])\n",
    "            #spectra = list(map(float,entry[1][:-2].split(',')))\n",
    "            #print(spectra)\n",
    "            \n",
    "            entry_dict = dict()\n",
    "            for i,col in enumerate(head[:-2]):\n",
    "                if col not in ['name','sequenceid','observationid','starttime']:\n",
    "                    entry_dict[col]=float(info[i])\n",
    "                else:\n",
    "                    entry_dict[col]=info[i]\n",
    "\n",
    "            \n",
    "            entry_dict['spectrum'] = np.array(spectra)\n",
    "            \n",
    "            f_ = fit(entry_dict['spectrum'])\n",
    "            \n",
    "            if f_ == {}:\n",
    "                bad+=1\n",
    "                print('bad fit',bad,entry_dict['name'])\n",
    "                continue\n",
    "                \n",
    "            entry_dict.update(f_)\n",
    "\n",
    "            data.append(entry_dict)\n",
    "            cube_name = entry_dict['name'].split('.cub')[0]\n",
    "            latitude = entry_dict['latitude']\n",
    "            longitude = entry_dict['longitude']\n",
    "            file = open(f\"pixels/{shape_name}/{shape_name}_{cube_name}_{latitude}_{longitude}.pkl\",\"wb\")\n",
    "            pickle.dump(entry_dict,file)\n",
    "            file.close()\n",
    "            count+=1\n",
    "            if count == 1:\n",
    "                print(data)\n",
    "                #break\n",
    "            if count%1000 == 0:\n",
    "                print(\"Progress\",count)\n",
    "            \n",
    "            #print('DONE')\n",
    "    print(count,bad)\n",
    "    #return data\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_csv('senkyo_pixels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_csv('belet_pixels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_csv('shangrai_pixels.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
